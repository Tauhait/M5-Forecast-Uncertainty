{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e5cb0b06",
   "metadata": {},
   "source": [
    "## Computing Predictions for Specific Dates and Time Horizons\n",
    "\n",
    "The plan for replicating Monsaraida’s solution involves creating a customizable Jupyter notebook with input parameters to process the data for training and test datasets and build LightGBM models for predictions. The objective is to train these models to predict values for a specific number of days into the future.\n",
    "\n",
    "To achieve the best results, the approach is to have each model learn to predict values for a specific week range in the future. As the task requires predicting up to 28 days ahead, the strategy is to create four different models:\n",
    "- Model 1: Predicts values from day +1 to day +7 in the future.\n",
    "- Model 2: Predicts values from day +8 to day +14 in the future.\n",
    "- Model 3: Predicts values from day +15 to +21 in the future.\n",
    "- Model 4: Predicts values from day +22 to day +28 in the future.\n",
    "\n",
    "Thus, a separate Kaggle notebook will be used for each of these time ranges, resulting in four notebooks in total. Each notebook will be trained to predict future time spans for each of the 10 stores that were part of the competition, producing ten models per notebook. Consequently, the four notebooks together will generate 40 models, covering all future ranges for all the stores.\n",
    "\n",
    "Since predictions need to be made for both the public leaderboard and the private one, the entire process is repeated twice:\n",
    "1. For the public test set submission, training stops at day 1,913, predicting days from 1,914 to 1,941.\n",
    "2. For the private test set submission, training stops at day 1,941, predicting days from 1,942 to 1,969.\n",
    "\n",
    "To expedite the process and optimize resource utilization, all eight notebooks can be run in parallel. However, it is crucial to differentiate each notebook based on its name, containing the parameter values relative to the last training day and the look-ahead horizon in days.\n",
    "\n",
    "The entire workflow, involving data preprocessing, model training, and predicting future values, will be executed across multiple notebooks, allowing for efficient and effective replication of Monsaraida’s solution for accurate time series forecasting on the Kaggle platform.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "25f86394",
   "metadata": {},
   "source": [
    "### import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cad4eafa",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-08-10T07:59:03.829558Z",
     "iopub.status.busy": "2022-08-10T07:59:03.828836Z",
     "iopub.status.idle": "2022-08-10T07:59:05.969686Z",
     "shell.execute_reply": "2022-08-10T07:59:05.968407Z"
    },
    "papermill": {
     "duration": 2.152967,
     "end_time": "2022-08-10T07:59:05.973058",
     "exception": false,
     "start_time": "2022-08-10T07:59:03.820091",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "from decimal import Decimal as dec\n",
    "import datetime\n",
    "import time\n",
    "import gc\n",
    "import lightgbm as lgb\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b3f6e092",
   "metadata": {},
   "source": [
    "### function to reduce the pandas DataFrame memory footprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e46d623",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-10T07:59:05.986369Z",
     "iopub.status.busy": "2022-08-10T07:59:05.985921Z",
     "iopub.status.idle": "2022-08-10T08:01:59.529797Z",
     "shell.execute_reply": "2022-08-10T08:01:59.528375Z"
    },
    "papermill": {
     "duration": 173.553756,
     "end_time": "2022-08-10T08:01:59.532604",
     "exception": false,
     "start_time": "2022-08-10T07:59:05.978848",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 96.13 Mb (78.8% reduction)\n",
      "Mem. usage decreased to 143.53 Mb (31.2% reduction)\n",
      "Mem. usage decreased to  0.12 Mb (41.9% reduction)\n",
      "Mem. usage decreased to  2.09 Mb (84.5% reduction)\n"
     ]
    }
   ],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "17b387fd",
   "metadata": {},
   "source": [
    "### function helps us to load all the data available and compress it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bd5473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    train_df = reduce_mem_usage(pd.read_csv(\"../data/m5-forecasting-accuracy/sales_train_evaluation.csv\"))\n",
    "    prices_df = reduce_mem_usage(pd.read_csv(\"../data/m5-forecasting-accuracy/sell_prices.csv\"))\n",
    "    calendar_df = reduce_mem_usage(pd.read_csv(\"../data/m5-forecasting-accuracy/calendar.csv\"))\n",
    "    submission_df = reduce_mem_usage(pd.read_csv(\"../data/m5-forecasting-accuracy/sample_submission.csv\"))\n",
    "    return train_df, prices_df, calendar_df, submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dc2b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, prices_df, calendar_df, submission_df = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d862bdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>weekday</th>\n",
       "      <th>wday</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>d</th>\n",
       "      <th>event_name_1</th>\n",
       "      <th>event_type_1</th>\n",
       "      <th>event_name_2</th>\n",
       "      <th>event_type_2</th>\n",
       "      <th>snap_CA</th>\n",
       "      <th>snap_TX</th>\n",
       "      <th>snap_WI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-30</td>\n",
       "      <td>11101</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-31</td>\n",
       "      <td>11101</td>\n",
       "      <td>Monday</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-02-01</td>\n",
       "      <td>11101</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-02-02</td>\n",
       "      <td>11101</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964</th>\n",
       "      <td>2016-06-15</td>\n",
       "      <td>11620</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>2016</td>\n",
       "      <td>d_1965</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1965</th>\n",
       "      <td>2016-06-16</td>\n",
       "      <td>11620</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2016</td>\n",
       "      <td>d_1966</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1966</th>\n",
       "      <td>2016-06-17</td>\n",
       "      <td>11620</td>\n",
       "      <td>Friday</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>2016</td>\n",
       "      <td>d_1967</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>2016-06-18</td>\n",
       "      <td>11621</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2016</td>\n",
       "      <td>d_1968</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1968</th>\n",
       "      <td>2016-06-19</td>\n",
       "      <td>11621</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2016</td>\n",
       "      <td>d_1969</td>\n",
       "      <td>NBAFinalsEnd</td>\n",
       "      <td>Sporting</td>\n",
       "      <td>Father's day</td>\n",
       "      <td>Cultural</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1969 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date  wm_yr_wk    weekday  wday  month  year       d  \\\n",
       "0     2011-01-29     11101   Saturday     1      1  2011     d_1   \n",
       "1     2011-01-30     11101     Sunday     2      1  2011     d_2   \n",
       "2     2011-01-31     11101     Monday     3      1  2011     d_3   \n",
       "3     2011-02-01     11101    Tuesday     4      2  2011     d_4   \n",
       "4     2011-02-02     11101  Wednesday     5      2  2011     d_5   \n",
       "...          ...       ...        ...   ...    ...   ...     ...   \n",
       "1964  2016-06-15     11620  Wednesday     5      6  2016  d_1965   \n",
       "1965  2016-06-16     11620   Thursday     6      6  2016  d_1966   \n",
       "1966  2016-06-17     11620     Friday     7      6  2016  d_1967   \n",
       "1967  2016-06-18     11621   Saturday     1      6  2016  d_1968   \n",
       "1968  2016-06-19     11621     Sunday     2      6  2016  d_1969   \n",
       "\n",
       "      event_name_1 event_type_1  event_name_2 event_type_2  snap_CA  snap_TX  \\\n",
       "0              NaN          NaN           NaN          NaN        0        0   \n",
       "1              NaN          NaN           NaN          NaN        0        0   \n",
       "2              NaN          NaN           NaN          NaN        0        0   \n",
       "3              NaN          NaN           NaN          NaN        1        1   \n",
       "4              NaN          NaN           NaN          NaN        1        0   \n",
       "...            ...          ...           ...          ...      ...      ...   \n",
       "1964           NaN          NaN           NaN          NaN        0        1   \n",
       "1965           NaN          NaN           NaN          NaN        0        0   \n",
       "1966           NaN          NaN           NaN          NaN        0        0   \n",
       "1967           NaN          NaN           NaN          NaN        0        0   \n",
       "1968  NBAFinalsEnd     Sporting  Father's day     Cultural        0        0   \n",
       "\n",
       "      snap_WI  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           1  \n",
       "...       ...  \n",
       "1964        1  \n",
       "1965        0  \n",
       "1966        0  \n",
       "1967        0  \n",
       "1968        0  \n",
       "\n",
       "[1969 rows x 14 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calendar_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8eb53b62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>sell_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CA_1</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>11325</td>\n",
       "      <td>9.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CA_1</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>11326</td>\n",
       "      <td>9.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CA_1</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>11327</td>\n",
       "      <td>8.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CA_1</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>11328</td>\n",
       "      <td>8.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CA_1</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>11329</td>\n",
       "      <td>8.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6841116</th>\n",
       "      <td>WI_3</td>\n",
       "      <td>FOODS_3_827</td>\n",
       "      <td>11617</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6841117</th>\n",
       "      <td>WI_3</td>\n",
       "      <td>FOODS_3_827</td>\n",
       "      <td>11618</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6841118</th>\n",
       "      <td>WI_3</td>\n",
       "      <td>FOODS_3_827</td>\n",
       "      <td>11619</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6841119</th>\n",
       "      <td>WI_3</td>\n",
       "      <td>FOODS_3_827</td>\n",
       "      <td>11620</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6841120</th>\n",
       "      <td>WI_3</td>\n",
       "      <td>FOODS_3_827</td>\n",
       "      <td>11621</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6841121 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        store_id        item_id  wm_yr_wk  sell_price\n",
       "0           CA_1  HOBBIES_1_001     11325        9.58\n",
       "1           CA_1  HOBBIES_1_001     11326        9.58\n",
       "2           CA_1  HOBBIES_1_001     11327        8.26\n",
       "3           CA_1  HOBBIES_1_001     11328        8.26\n",
       "4           CA_1  HOBBIES_1_001     11329        8.26\n",
       "...          ...            ...       ...         ...\n",
       "6841116     WI_3    FOODS_3_827     11617        1.00\n",
       "6841117     WI_3    FOODS_3_827     11618        1.00\n",
       "6841118     WI_3    FOODS_3_827     11619        1.00\n",
       "6841119     WI_3    FOODS_3_827     11620        1.00\n",
       "6841120     WI_3    FOODS_3_827     11621        1.00\n",
       "\n",
       "[6841121 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba29149f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d_1</th>\n",
       "      <th>d_2</th>\n",
       "      <th>d_3</th>\n",
       "      <th>d_4</th>\n",
       "      <th>...</th>\n",
       "      <th>d_1932</th>\n",
       "      <th>d_1933</th>\n",
       "      <th>d_1934</th>\n",
       "      <th>d_1935</th>\n",
       "      <th>d_1936</th>\n",
       "      <th>d_1937</th>\n",
       "      <th>d_1938</th>\n",
       "      <th>d_1939</th>\n",
       "      <th>d_1940</th>\n",
       "      <th>d_1941</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_002</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_003</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_004</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_005</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30485</th>\n",
       "      <td>FOODS_3_823_WI_3_evaluation</td>\n",
       "      <td>FOODS_3_823</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>WI_3</td>\n",
       "      <td>WI</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30486</th>\n",
       "      <td>FOODS_3_824_WI_3_evaluation</td>\n",
       "      <td>FOODS_3_824</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>WI_3</td>\n",
       "      <td>WI</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30487</th>\n",
       "      <td>FOODS_3_825_WI_3_evaluation</td>\n",
       "      <td>FOODS_3_825</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>WI_3</td>\n",
       "      <td>WI</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30488</th>\n",
       "      <td>FOODS_3_826_WI_3_evaluation</td>\n",
       "      <td>FOODS_3_826</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>WI_3</td>\n",
       "      <td>WI</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30489</th>\n",
       "      <td>FOODS_3_827_WI_3_evaluation</td>\n",
       "      <td>FOODS_3_827</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>WI_3</td>\n",
       "      <td>WI</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30490 rows × 1947 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  id        item_id    dept_id   cat_id  \\\n",
       "0      HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES   \n",
       "1      HOBBIES_1_002_CA_1_evaluation  HOBBIES_1_002  HOBBIES_1  HOBBIES   \n",
       "2      HOBBIES_1_003_CA_1_evaluation  HOBBIES_1_003  HOBBIES_1  HOBBIES   \n",
       "3      HOBBIES_1_004_CA_1_evaluation  HOBBIES_1_004  HOBBIES_1  HOBBIES   \n",
       "4      HOBBIES_1_005_CA_1_evaluation  HOBBIES_1_005  HOBBIES_1  HOBBIES   \n",
       "...                              ...            ...        ...      ...   \n",
       "30485    FOODS_3_823_WI_3_evaluation    FOODS_3_823    FOODS_3    FOODS   \n",
       "30486    FOODS_3_824_WI_3_evaluation    FOODS_3_824    FOODS_3    FOODS   \n",
       "30487    FOODS_3_825_WI_3_evaluation    FOODS_3_825    FOODS_3    FOODS   \n",
       "30488    FOODS_3_826_WI_3_evaluation    FOODS_3_826    FOODS_3    FOODS   \n",
       "30489    FOODS_3_827_WI_3_evaluation    FOODS_3_827    FOODS_3    FOODS   \n",
       "\n",
       "      store_id state_id  d_1  d_2  d_3  d_4  ...  d_1932  d_1933  d_1934  \\\n",
       "0         CA_1       CA    0    0    0    0  ...       2       4       0   \n",
       "1         CA_1       CA    0    0    0    0  ...       0       1       2   \n",
       "2         CA_1       CA    0    0    0    0  ...       1       0       2   \n",
       "3         CA_1       CA    0    0    0    0  ...       1       1       0   \n",
       "4         CA_1       CA    0    0    0    0  ...       0       0       0   \n",
       "...        ...      ...  ...  ...  ...  ...  ...     ...     ...     ...   \n",
       "30485     WI_3       WI    0    0    2    2  ...       1       0       3   \n",
       "30486     WI_3       WI    0    0    0    0  ...       0       0       0   \n",
       "30487     WI_3       WI    0    6    0    2  ...       0       0       1   \n",
       "30488     WI_3       WI    0    0    0    0  ...       1       1       1   \n",
       "30489     WI_3       WI    0    0    0    0  ...       1       2       0   \n",
       "\n",
       "       d_1935  d_1936  d_1937  d_1938  d_1939  d_1940  d_1941  \n",
       "0           0       0       0       3       3       0       1  \n",
       "1           1       1       0       0       0       0       0  \n",
       "2           0       0       0       2       3       0       1  \n",
       "3           4       0       1       3       0       2       6  \n",
       "4           2       1       0       0       2       1       0  \n",
       "...       ...     ...     ...     ...     ...     ...     ...  \n",
       "30485       0       1       1       0       0       1       1  \n",
       "30486       0       0       0       1       0       1       0  \n",
       "30487       2       0       1       0       1       0       2  \n",
       "30488       4       6       0       1       1       1       0  \n",
       "30489       5       4       0       2       2       5       1  \n",
       "\n",
       "[30490 rows x 1947 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb762859",
   "metadata": {},
   "source": [
    "### check usage of melt function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "750c2b7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>variable</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_002</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_003</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_004</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_005</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59181085</th>\n",
       "      <td>FOODS_3_823_WI_3_evaluation</td>\n",
       "      <td>FOODS_3_823</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>WI_3</td>\n",
       "      <td>WI</td>\n",
       "      <td>d_1941</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59181086</th>\n",
       "      <td>FOODS_3_824_WI_3_evaluation</td>\n",
       "      <td>FOODS_3_824</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>WI_3</td>\n",
       "      <td>WI</td>\n",
       "      <td>d_1941</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59181087</th>\n",
       "      <td>FOODS_3_825_WI_3_evaluation</td>\n",
       "      <td>FOODS_3_825</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>WI_3</td>\n",
       "      <td>WI</td>\n",
       "      <td>d_1941</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59181088</th>\n",
       "      <td>FOODS_3_826_WI_3_evaluation</td>\n",
       "      <td>FOODS_3_826</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>WI_3</td>\n",
       "      <td>WI</td>\n",
       "      <td>d_1941</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59181089</th>\n",
       "      <td>FOODS_3_827_WI_3_evaluation</td>\n",
       "      <td>FOODS_3_827</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>WI_3</td>\n",
       "      <td>WI</td>\n",
       "      <td>d_1941</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59181090 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id        item_id    dept_id   cat_id  \\\n",
       "0         HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES   \n",
       "1         HOBBIES_1_002_CA_1_evaluation  HOBBIES_1_002  HOBBIES_1  HOBBIES   \n",
       "2         HOBBIES_1_003_CA_1_evaluation  HOBBIES_1_003  HOBBIES_1  HOBBIES   \n",
       "3         HOBBIES_1_004_CA_1_evaluation  HOBBIES_1_004  HOBBIES_1  HOBBIES   \n",
       "4         HOBBIES_1_005_CA_1_evaluation  HOBBIES_1_005  HOBBIES_1  HOBBIES   \n",
       "...                                 ...            ...        ...      ...   \n",
       "59181085    FOODS_3_823_WI_3_evaluation    FOODS_3_823    FOODS_3    FOODS   \n",
       "59181086    FOODS_3_824_WI_3_evaluation    FOODS_3_824    FOODS_3    FOODS   \n",
       "59181087    FOODS_3_825_WI_3_evaluation    FOODS_3_825    FOODS_3    FOODS   \n",
       "59181088    FOODS_3_826_WI_3_evaluation    FOODS_3_826    FOODS_3    FOODS   \n",
       "59181089    FOODS_3_827_WI_3_evaluation    FOODS_3_827    FOODS_3    FOODS   \n",
       "\n",
       "         store_id state_id variable  value  \n",
       "0            CA_1       CA      d_1      0  \n",
       "1            CA_1       CA      d_1      0  \n",
       "2            CA_1       CA      d_1      0  \n",
       "3            CA_1       CA      d_1      0  \n",
       "4            CA_1       CA      d_1      0  \n",
       "...           ...      ...      ...    ...  \n",
       "59181085     WI_3       WI   d_1941      1  \n",
       "59181086     WI_3       WI   d_1941      0  \n",
       "59181087     WI_3       WI   d_1941      2  \n",
       "59181088     WI_3       WI   d_1941      0  \n",
       "59181089     WI_3       WI   d_1941      1  \n",
       "\n",
       "[59181090 rows x 8 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_columns = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n",
    "temp_df = pd.melt(train_df, id_vars=index_columns)\n",
    "temp_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6457b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del(temp_df)\n",
    "gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e55aaba8",
   "metadata": {},
   "source": [
    "## Function: generate_base_grid(train_df, end_train_day_x, predict_horizon)\n",
    "\n",
    "This function is designed to generate a base grid for time series data based on the input DataFrame `train_df`, a specific end day `end_train_day_x`, and the number of days to predict into the future `predict_horizon`.\n",
    "\n",
    "1. **Data Preprocessing:**\n",
    "   - The `index_columns` list is defined, which contains the column names that will be used as index columns for the grid.\n",
    "   - The `train_df` DataFrame is transformed using the `pd.melt` function. The data is converted from a wide format to a long format, with the 'd' column representing the day identifier and the 'sales' column representing the sales data.\n",
    "   - The `reduce_mem_usage` function is called to reduce the memory usage of the DataFrame `grid_df` after the transformation.\n",
    "\n",
    "2. **Create Holdout Data:**\n",
    "   - The 'd' column in `grid_df` is temporarily stored in a new column 'd_org' before being modified.\n",
    "   - The 'd' column in `grid_df` is updated by removing the first two characters from each element and converting the resulting values to 16-bit integers. This is done to convert day identifiers from the format 'd_x' to numerical values.\n",
    "   - A boolean mask `time_mask` is created based on the 'd' column, which filters rows where the day identifier is within the range from `end_train_day_x` to `end_train_day_x + predict_horizon`.\n",
    "   - A new DataFrame `holdout_df` is created by selecting specific columns (\"id\", \"d\", and \"sales\") from `grid_df` using the boolean mask `time_mask`. The index of `holdout_df` is reset, and the resulting data is saved as a Feather file named based on the value of `end_train_day_x` and `end_train_day_x + predict_horizon`.\n",
    "\n",
    "3. **Update Grid Data:**\n",
    "   - The original `grid_df` is updated by keeping only the rows where the 'd' column is less than or equal to `end_train_day_x`. This step essentially removes data for days after `end_train_day_x`.\n",
    "   - The 'd' column in `grid_df` is restored to its original values using the 'd_org' column.\n",
    "   - The 'd_org' column is dropped from `grid_df` as it is no longer needed.\n",
    "\n",
    "4. **Create Additional Grid Data:**\n",
    "   - A new DataFrame `add_grid` is created to hold additional data for future days.\n",
    "   - A loop is executed for each day in the range specified by `predict_horizon`.\n",
    "   - A temporary DataFrame `temp_df` is created by selecting only the columns in `index_columns` from `train_df` and removing duplicate rows.\n",
    "   - A new 'd' column is added to `temp_df` with unique day identifiers in the format 'd_x', where 'x' is calculated as `end_train_day_x + i + 1`.\n",
    "   - A new 'sales' column is added to `temp_df`, and all values are initialized as NaN, indicating that the sales data for these future days is unknown and needs to be predicted or filled later.\n",
    "   - The temporary DataFrame `temp_df` is concatenated to `add_grid`, accumulating data for each day into the future.\n",
    "\n",
    "5. **Combine Grid Data:**\n",
    "   - The original `grid_df` and the newly created `add_grid` are concatenated to create a combined DataFrame, including historical and future data.\n",
    "   - The index of the combined DataFrame is reset to ensure consecutive integer index values.\n",
    "\n",
    "6. **Data Type Conversion:**\n",
    "   - The data type of each column in `grid_df` specified in `index_columns` is converted to the 'category' data type. This can lead to memory optimization and faster operations for categorical data.\n",
    "\n",
    "7. **Final Data Preparation and Saving:**\n",
    "   - The memory usage of `grid_df` is reduced using the `reduce_mem_usage` function.\n",
    "   - The final `grid_df` data is saved as a Feather file named based on the value of `end_train_day_x` and `end_train_day_x + predict_horizon`.\n",
    "\n",
    "8. **Memory Management:**\n",
    "   - The function releases the memory occupied by the `holdout_df` and `grid_df` DataFrames using `del` and `gc.collect()` to ensure efficient memory usage.\n",
    "\n",
    "9. **Print Result:**\n",
    "   - The function prints the first few rows of the `grid_df` DataFrame using `print(grid_df.head())`.\n",
    "\n",
    "The `generate_base_grid` function performs data preprocessing, creates a holdout dataset for future predictions, updates the main grid with historical data, adds future data, and saves the final grid data as Feather files for further analysis and modeling in time series forecasting tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "774c09fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-10T08:01:59.544310Z",
     "iopub.status.busy": "2022-08-10T08:01:59.543921Z",
     "iopub.status.idle": "2022-08-10T08:01:59.556478Z",
     "shell.execute_reply": "2022-08-10T08:01:59.555600Z"
    },
    "papermill": {
     "duration": 0.02097,
     "end_time": "2022-08-10T08:01:59.558672",
     "exception": false,
     "start_time": "2022-08-10T08:01:59.537702",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_base_grid(train_df, end_train_day_x, predict_horizon):\n",
    "    index_columns = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n",
    "\n",
    "    grid_df = pd.melt(train_df, id_vars=index_columns, var_name='d', value_name='sales')\n",
    "    grid_df = reduce_mem_usage(grid_df, verbose=False)\n",
    "\n",
    "    grid_df['d_org'] = grid_df['d']\n",
    "    # removes the first two characters from each element in the 'd' column\n",
    "    # first two characters (\"d_\") have been removed, and the resulting values are converted to 16-bit integers\n",
    "    grid_df['d'] = grid_df['d'].apply(lambda x: x[2:]).astype(np.int16)\n",
    "\n",
    "    time_mask = (grid_df['d'] > end_train_day_x) &  (grid_df['d'] <= end_train_day_x + predict_horizon)\n",
    "    # # creates a new DataFrame holdout_df by filtering grid_df based on the boolean mask time_mask, keeping only columns \"id\", \"d\", and \"sales\" and resetting the index.\n",
    "    holdout_df = grid_df.loc[time_mask, [\"id\", \"d\", \"sales\"]].reset_index(drop=True)\n",
    "    #  lightweight format ideal for fast I/O and interoperability\n",
    "    holdout_df.to_feather(f\"holdout_df_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather\")\n",
    "    del(holdout_df)\n",
    "    gc.collect()\n",
    "\n",
    "    grid_df = grid_df[grid_df['d'] <= end_train_day_x]\n",
    "    grid_df['d'] = grid_df['d_org']\n",
    "    grid_df = grid_df.drop('d_org', axis=1)\n",
    "\n",
    "    add_grid = pd.DataFrame()\n",
    "    for i in range(predict_horizon):\n",
    "        temp_df = train_df[index_columns]\n",
    "        temp_df = temp_df.drop_duplicates()\n",
    "        temp_df['d'] = 'd_' + str(end_train_day_x + i + 1)\n",
    "        temp_df['sales'] = np.nan\n",
    "        add_grid = pd.concat([add_grid, temp_df])\n",
    "    \n",
    "    grid_df = pd.concat([grid_df, add_grid])\n",
    "    grid_df = grid_df.reset_index(drop=True)\n",
    "    \n",
    "    for col in index_columns:\n",
    "        grid_df[col] = grid_df[col].astype('category')\n",
    "    \n",
    "    grid_df = reduce_mem_usage(grid_df, verbose=False)\n",
    "    grid_df.to_feather(f\"grid_df_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather\")\n",
    "    print(grid_df.head())\n",
    "    del(grid_df)\n",
    "    gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab714c79",
   "metadata": {},
   "source": [
    "## Function Explanation: merge_by_concat\n",
    "\n",
    "This function is designed to merge two pandas DataFrames (`df1` and `df2`) based on a common set of columns specified by the `merge_on` parameter. It uses the `pd.merge` function to perform the merge operation and then concatenates the results back to the original DataFrame `df1`. The function returns the merged DataFrame.\n",
    "\n",
    "### Function Parameters:\n",
    "- `df1`: The first pandas DataFrame to be merged.\n",
    "- `df2`: The second pandas DataFrame to be merged with `df1`.\n",
    "- `merge_on`: A list of column names representing the common columns used for merging the two DataFrames.\n",
    "\n",
    "### Function Steps:\n",
    "\n",
    "1. **Extract the Columns to be Merged:**\n",
    "   - `merged_gf = df1[merge_on]`: Creates a new DataFrame `merged_gf` containing only the columns specified in the `merge_on` list. This DataFrame serves as an intermediate representation of the columns to be merged.\n",
    "\n",
    "2. **Merge the DataFrames:**\n",
    "   - `merged_gf = merged_gf.merge(df2, on=merge_on, how='left')`: Performs a left-join merge of `merged_gf` with the DataFrame `df2` based on the columns specified in the `merge_on` list. The `how='left'` parameter ensures that all rows from `merged_gf` are retained, and matching rows from `df2` are appended to it based on the common columns.\n",
    "\n",
    "3. **Extract New Columns:**\n",
    "   - `new_columns = [col for col in list(merged_gf) if col not in merge_on]`: Identifies the columns in `merged_gf` that are not part of the `merge_on` list. These columns represent the newly merged data from `df2`.\n",
    "\n",
    "4. **Concatenate Merged Data with Original DataFrame:**\n",
    "   - `df1 = pd.concat([df1, merged_gf[new_columns]], axis=1)`: Concatenates the original DataFrame `df1` with the columns in `merged_gf` that are not part of the `merge_on` list. This effectively adds the merged data from `df2` to `df1`.\n",
    "\n",
    "5. **Return the Merged DataFrame:**\n",
    "   - `return df1`: The function returns the updated DataFrame `df1`, which now includes the merged data from `df2` based on the specified `merge_on` columns.\n",
    "\n",
    "### Use Case:\n",
    "The `merge_by_concat` function can be particularly useful when you want to merge two DataFrames based on specific columns, but you also want to retain all the original data from the first DataFrame (`df1`). By using a left-join merge strategy and concatenating the results back to `df1`, this function ensures that no data from the original DataFrame is lost during the merge process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4da67bef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-10T08:01:59.570875Z",
     "iopub.status.busy": "2022-08-10T08:01:59.570208Z",
     "iopub.status.idle": "2022-08-10T08:01:59.580610Z",
     "shell.execute_reply": "2022-08-10T08:01:59.579384Z"
    },
    "papermill": {
     "duration": 0.019454,
     "end_time": "2022-08-10T08:01:59.582989",
     "exception": false,
     "start_time": "2022-08-10T08:01:59.563535",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_by_concat(df1, df2, merge_on):\n",
    "    merged_gf = df1[merge_on]\n",
    "    merged_gf = merged_gf.merge(df2, on=merge_on, how='left')\n",
    "    new_columns = [col for col in list(merged_gf) if col not in merge_on]\n",
    "    df1 = pd.concat([df1, merged_gf[new_columns]], axis=1)\n",
    "    return df1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1677944a",
   "metadata": {},
   "source": [
    "## Function: calc_release_week(prices_df, end_train_day_x, predict_horizon)\n",
    "\n",
    "This function calculates the \"release week\" for items in a given DataFrame and updates the data for a specific range of days. It performs the following steps:\n",
    "\n",
    "1. **Input Parameters**:\n",
    "   - `prices_df`: A DataFrame containing information about item prices, including columns like 'store_id', 'item_id', 'wm_yr_wk' (representing the year and week), and other relevant information.\n",
    "   - `end_train_day_x`: The last day of the training data. The function will calculate the release week up to this day.\n",
    "   - `predict_horizon`: The number of days into the future to predict. The function will update the data for this future prediction horizon.\n",
    "\n",
    "2. **Extracting Index Columns**:\n",
    "   - `index_columns`: A list containing the column names used as index or grouping columns in the DataFrame.\n",
    "\n",
    "3. **Reading the Data**:\n",
    "   - The function reads a Feather file named `grid_df_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather`, which presumably contains data for a specific range of days.\n",
    "\n",
    "4. **Calculating the Release Week**:\n",
    "   - It groups the `prices_df` by 'store_id' and 'item_id' and calculates the minimum 'wm_yr_wk' (year and week identifier) for each item, representing the release week of that item.\n",
    "   - The result is stored in a new DataFrame called `release_df`, with columns 'store_id', 'item_id', and 'release', representing the release week.\n",
    "\n",
    "5. **Merging the Release Week Information**:\n",
    "   - The function merges the `release_df` DataFrame into the original `grid_df` DataFrame, adding the 'release' column to the latter. The merge is done based on the common columns 'store_id' and 'item_id'.\n",
    "\n",
    "6. **Memory Optimization**:\n",
    "   - Unnecessary intermediate variables are removed to free up memory.\n",
    "   - The `reduce_mem_usage` function is applied to optimize memory usage of the DataFrame `grid_df`.\n",
    "\n",
    "7. **Merging Calendar Information**:\n",
    "   - The `calendar_df` DataFrame, containing columns 'wm_yr_wk' (year and week identifier) and 'd' (day identifier), is used to merge additional calendar information into the `grid_df` DataFrame based on the common column 'd'.\n",
    "\n",
    "8. **Calculating Relative Release Week**:\n",
    "   - The 'release' column in `grid_df` is adjusted to represent the relative release week. It calculates the difference between the 'release' value and the minimum 'release' value and stores the result back in the 'release' column.\n",
    "\n",
    "9. **Memory Optimization (Again)**:\n",
    "   - The function optimizes the memory usage of the `grid_df` DataFrame once more using the `reduce_mem_usage` function.\n",
    "\n",
    "10. **Saving Data**:\n",
    "    - The updated DataFrame `grid_df` is saved as a Feather file named `grid_df_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather`. This file now contains the updated release week information for the given range of days.\n",
    "\n",
    "11. **Cleaning Up**:\n",
    "    - Intermediate variables and DataFrames are deleted to free up memory.\n",
    "    - Garbage collection is invoked to further release memory resources.\n",
    "\n",
    "Note: Some functions like `merge_by_concat` and `reduce_mem_usage` are used in the function, but their definitions are not provided in this code snippet. These functions likely handle merging DataFrames and optimizing memory usage, respectively.\n",
    "\n",
    "The purpose of this function is to update the `grid_df` DataFrame with the calculated release week information and other relevant data for a given range of days, making it ready for further analysis or modeling tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "daa12234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_release_week(prices_df, end_train_day_x, predict_horizon):\n",
    "    index_columns = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n",
    "    \n",
    "    grid_df = pd.read_feather(f\"grid_df_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather\")\n",
    "    \n",
    "    release_df = prices_df.groupby(['store_id', 'item_id'])['wm_yr_wk'].agg(['min']).reset_index()\n",
    "    release_df.columns = ['store_id', 'item_id', 'release']\n",
    "    \n",
    "    grid_df = merge_by_concat(grid_df, release_df, ['store_id', 'item_id'])\n",
    "    \n",
    "    del release_df\n",
    "    grid_df = reduce_mem_usage(grid_df, verbose=False)\n",
    "    gc.collect()\n",
    "    \n",
    "    grid_df = merge_by_concat(grid_df, calendar_df[['wm_yr_wk', 'd']], ['d'])\n",
    "    grid_df = grid_df.reset_index(drop=True)\n",
    "\n",
    "    grid_df['release'] = grid_df['release'] - grid_df['release'].min()\n",
    "    grid_df['release'] = grid_df['release'].astype(np.int16)\n",
    "    \n",
    "    grid_df = reduce_mem_usage(grid_df, verbose=False)\n",
    "    grid_df.to_feather(f\"grid_df_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather\")\n",
    "    del(grid_df)\n",
    "    gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "276267e3",
   "metadata": {},
   "source": [
    "### Function: `generate_grid_price(prices_df, calendar_df, end_train_day_x, predict_horizon)`\n",
    "\n",
    "1. **Input Parameters**:\n",
    "   - `prices_df`: DataFrame containing historical sell prices for different items in various stores.\n",
    "   - `calendar_df`: DataFrame containing calendar information, such as week number (`wm_yr_wk`), month, and year.\n",
    "   - `end_train_day_x`: The last day of the training data.\n",
    "   - `predict_horizon`: The number of days into the future to predict.\n",
    "\n",
    "2. **Reading the Existing Grid Data**:\n",
    "   - The function reads the existing grid DataFrame (`grid_df`) from a Feather file named `\"grid_df_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather\"`.\n",
    "\n",
    "3. **Calculating Price-Related Features**:\n",
    "   - The function computes several price-related features for the `prices_df` DataFrame, including 'price_max', 'price_min', 'price_std', 'price_mean', 'price_norm', 'price_nunique', and 'item_nunique'. These features are calculated by grouping data based on store ID and item ID and performing statistical operations on the 'sell_price' column.\n",
    "\n",
    "4. **Merging Calendar Information with Prices Data**:\n",
    "   - The function extracts relevant calendar information (`'wm_yr_wk'`, `'month'`, and `'year'`) from `calendar_df` and drops duplicates based on 'wm_yr_wk'.\n",
    "   - The `prices_df` DataFrame is then merged with the extracted calendar information on 'wm_yr_wk' to add month and year information for each sell price entry.\n",
    "\n",
    "5. **Calculating Additional Price Features**:\n",
    "   - The function computes additional price-related features, such as 'price_momentum', 'price_momentum_m', and 'price_momentum_y', which involve measuring the momentum of sell prices based on the previous day, previous month, and previous year, respectively.\n",
    "\n",
    "6. **Extracting Decimal Part of Prices**:\n",
    "   - The function extracts the decimal part (cent) of sell prices, 'price_max', and 'price_min' using the `math.modf()` function.\n",
    "\n",
    "7. **Reducing Memory Usage**:\n",
    "   - The function optimizes memory usage by reducing the memory footprint of the `prices_df` DataFrame using the `reduce_mem_usage()` function.\n",
    "\n",
    "8. **Merging Prices Data with Existing Grid**:\n",
    "   - The function merges the `prices_df` DataFrame with the existing grid (`grid_df`) based on store ID, item ID, and week number ('wm_yr_wk').\n",
    "\n",
    "9. **Selecting Relevant Columns**:\n",
    "   - The function selects only the necessary columns ('id', 'd', and other price-related features) from the merged DataFrame.\n",
    "\n",
    "10. **Reducing Memory Usage for the Final Grid**:\n",
    "   - The function further optimizes memory usage for the final grid DataFrame by reducing its memory footprint.\n",
    "\n",
    "11. **Saving the Updated Grid to a Feather File**:\n",
    "   - The function saves the updated grid DataFrame as a new Feather file named `\"grid_price_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather\"`.\n",
    "\n",
    "12. **Memory Management**:\n",
    "   - The function releases memory occupied by intermediate DataFrames and variables using `del` statements and the `gc.collect()` function.\n",
    "\n",
    "### Output:\n",
    "The function performs various data processing and feature engineering steps to create a new grid DataFrame with additional price-related features, and saves the updated grid as a Feather file. The new grid includes data for historical days up to `end_train_day_x` and future days for a range defined by `predict_horizon`, with calculated features based on historical sell prices for different items in various stores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7a42064",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-10T08:01:59.594724Z",
     "iopub.status.busy": "2022-08-10T08:01:59.594229Z",
     "iopub.status.idle": "2022-08-10T08:01:59.611474Z",
     "shell.execute_reply": "2022-08-10T08:01:59.610004Z"
    },
    "papermill": {
     "duration": 0.026573,
     "end_time": "2022-08-10T08:01:59.614386",
     "exception": false,
     "start_time": "2022-08-10T08:01:59.587813",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_grid_price(prices_df, calendar_df, end_train_day_x, predict_horizon):\n",
    "\n",
    "    grid_df = pd.read_feather(f\"grid_df_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather\")\n",
    "\n",
    "    prices_df['price_max'] = prices_df.groupby(['store_id', 'item_id'])['sell_price'].transform('max')\n",
    "    prices_df['price_min'] = prices_df.groupby(['store_id', 'item_id'])['sell_price'].transform('min')\n",
    "    prices_df['price_std'] = prices_df.groupby(['store_id', 'item_id'])['sell_price'].transform('std')\n",
    "    prices_df['price_mean'] = prices_df.groupby(['store_id', 'item_id'])['sell_price'].transform('mean')\n",
    "    prices_df['price_norm'] = prices_df['sell_price'] / prices_df['price_max']\n",
    "    prices_df['price_nunique'] = prices_df.groupby(['store_id', 'item_id'])['sell_price'].transform('nunique')\n",
    "    prices_df['item_nunique'] = prices_df.groupby(['store_id', 'sell_price'])['item_id'].transform('nunique')\n",
    "\n",
    "    calendar_prices = calendar_df[['wm_yr_wk', 'month', 'year']]\n",
    "    calendar_prices = calendar_prices.drop_duplicates(subset=['wm_yr_wk'])\n",
    "    prices_df = prices_df.merge(calendar_prices[['wm_yr_wk', 'month', 'year']], on=['wm_yr_wk'], how='left')\n",
    "    \n",
    "    del calendar_prices\n",
    "    gc.collect()\n",
    "    \n",
    "    prices_df['price_momentum'] = prices_df['sell_price'] / prices_df.groupby(['store_id', 'item_id'])[\n",
    "        'sell_price'].transform(lambda x: x.shift(1))\n",
    "    prices_df['price_momentum_m'] = prices_df['sell_price'] / prices_df.groupby(['store_id', 'item_id', 'month'])[\n",
    "        'sell_price'].transform('mean')\n",
    "    prices_df['price_momentum_y'] = prices_df['sell_price'] / prices_df.groupby(['store_id', 'item_id', 'year'])[\n",
    "        'sell_price'].transform('mean')\n",
    "\n",
    "    prices_df['sell_price_cent'] = [math.modf(p)[0] for p in prices_df['sell_price']]\n",
    "    prices_df['price_max_cent'] = [math.modf(p)[0] for p in prices_df['price_max']]\n",
    "    prices_df['price_min_cent'] = [math.modf(p)[0] for p in prices_df['price_min']]\n",
    "\n",
    "    del prices_df['month'], prices_df['year']\n",
    "    prices_df = reduce_mem_usage(prices_df, verbose=False)\n",
    "    gc.collect()\n",
    "    \n",
    "    original_columns = list(grid_df)\n",
    "    grid_df = grid_df.merge(prices_df, on=['store_id', 'item_id', 'wm_yr_wk'], how='left')\n",
    "    del(prices_df)\n",
    "    gc.collect()\n",
    "    \n",
    "    keep_columns = [col for col in list(grid_df) if col not in original_columns]\n",
    "    grid_df = grid_df[['id', 'd'] + keep_columns]\n",
    "    grid_df = reduce_mem_usage(grid_df, verbose=False)\n",
    "\n",
    "    grid_df.to_feather(f\"grid_price_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather\")\n",
    "    del(grid_df)\n",
    "    gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a0e87877",
   "metadata": {},
   "source": [
    "### Function: get_moon_phase(d)\n",
    "This function calculates the moon phase index (0=new moon, 4=full moon) for a given date 'd'.\n",
    "\n",
    "### Parameters:\n",
    "- `d`: The input date for which the moon phase needs to be determined. It should be in the format 'YYYY-MM-DD'.\n",
    "\n",
    "### Dependencies:\n",
    "- The function uses the `datetime` module to handle date calculations and conversions.\n",
    "- It also requires the `decimal` module for precise decimal arithmetic and the `math` module for mathematical operations.\n",
    "\n",
    "### Algorithm:\n",
    "1. Convert the input date 'd' into the number of days since January 1, 2001, using the difference between the input date and January 1, 2001.\n",
    "2. Convert the number of days into a decimal value to handle precise arithmetic.\n",
    "3. Calculate the number of lunations (complete lunar cycles) that have occurred since January 1, 2001, using a formula that takes into account the time elapsed in days.\n",
    "4. Calculate the phase index by dividing the fractional part of the lunations by 1 and then multiplying it by 8. Round the result to the nearest whole number (0.5 is added before rounding to ensure correct rounding behavior).\n",
    "5. The phase index ranges from 0 to 7. To ensure the result is within this range, take the bitwise AND operation with 7.\n",
    "6. Return the final moon phase index as an integer.\n",
    "\n",
    "### Moon Phases:\n",
    "- 0: New Moon\n",
    "- 1: Waxing Crescent\n",
    "- 2: First Quarter\n",
    "- 3: Waxing Gibbous\n",
    "- 4: Full Moon\n",
    "- 5: Waning Gibbous\n",
    "- 6: Last Quarter\n",
    "- 7: Waning Crescent\n",
    "\n",
    "### Example Usage:\n",
    "```python\n",
    "date = \"2023-07-31\"\n",
    "moon_phase_index = get_moon_phase(date)\n",
    "print(\"Moon Phase Index for\", date, \":\", moon_phase_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96e491a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-10T08:01:59.627021Z",
     "iopub.status.busy": "2022-08-10T08:01:59.626501Z",
     "iopub.status.idle": "2022-08-10T08:01:59.642382Z",
     "shell.execute_reply": "2022-08-10T08:01:59.641152Z"
    },
    "papermill": {
     "duration": 0.025158,
     "end_time": "2022-08-10T08:01:59.644751",
     "exception": false,
     "start_time": "2022-08-10T08:01:59.619593",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_moon_phase(d):  # 0=new, 4=full; 4 days/phase\n",
    "    diff = datetime.datetime.strptime(d, '%Y-%m-%d') - datetime.datetime(2001, 1, 1)\n",
    "    days = dec(diff.days) + (dec(diff.seconds) / dec(86400))\n",
    "    lunations = dec(\"0.20439731\") + (days * dec(\"0.03386319269\"))\n",
    "    phase_index = math.floor((lunations % dec(1) * dec(8)) + dec('0.5'))\n",
    "    return int(phase_index) & 7\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3db6fd0",
   "metadata": {},
   "source": [
    "### Function Explanation: generate_grid_calendar\n",
    "\n",
    "This function generates a grid-based calendar DataFrame by merging a pre-existing `calendar_df` with a previously generated `grid_df`, and then performs various data preprocessing steps to prepare the resulting `grid_df` for further analysis or modeling.\n",
    "\n",
    "#### Arguments:\n",
    "- `calendar_df`: A DataFrame containing the calendar information, such as dates, events, and snap days (binary indicators of SNAP purchases) for each day.\n",
    "- `end_train_day_x`: An integer representing the last day of the training data.\n",
    "- `predict_horizon`: An integer representing the prediction horizon or the number of days to predict into the future.\n",
    "\n",
    "#### Steps:\n",
    "\n",
    "1. **Read Existing Grid DataFrame:**\n",
    "   - The function reads a previously generated Feather file named `grid_df_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather` to obtain the existing `grid_df`.\n",
    "\n",
    "2. **Select Columns:**\n",
    "   - The function selects only the 'id' and 'd' columns from the existing `grid_df` and assigns the result to `grid_df`.\n",
    "\n",
    "3. **Memory Management:**\n",
    "   - The function uses `gc.collect()` to perform garbage collection, freeing up memory.\n",
    "\n",
    "4. **Calculate Moon Phase:**\n",
    "   - The function calls the `get_moon_phase` function on the 'date' column of `calendar_df` to determine the moon phase for each date. The result is added to a new column named 'moon' in `calendar_df`.\n",
    "\n",
    "5. **Merge Calendar Information:**\n",
    "   - A subset of columns from `calendar_df` is merged with `grid_df` based on the 'd' (day identifier) column, using a left join. The selected columns include 'date', 'd', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', and the newly created 'moon' column.\n",
    "\n",
    "6. **Data Type Conversion:**\n",
    "   - Several categorical columns in `grid_df` (e.g., event names, event types, snap indicators) are converted to the 'category' data type to optimize memory usage.\n",
    "\n",
    "7. **Date Feature Engineering:**\n",
    "   - The 'date' column in `grid_df` is converted to a datetime data type using `pd.to_datetime`.\n",
    "   - New temporal features are derived from the 'date' column, including 'tm_d' (day of the month), 'tm_w' (ISO week of the year), 'tm_m' (month), 'tm_y' (year), 'tm_y' (year relative to the minimum year in the dataset), and 'tm_wm' (week of the month).\n",
    "   - Additionally, 'tm_dw' (day of the week) and 'tm_w_end' (binary indicator for weekend) features are created.\n",
    "\n",
    "8. **Memory Optimization and Saving:**\n",
    "   - Unnecessary columns (e.g., 'date') are removed from `grid_df` to save memory.\n",
    "   - The function calls `reduce_mem_usage` to optimize the memory usage of `grid_df`.\n",
    "   - The resulting `grid_df` is saved as a new Feather file named `grid_calendar_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather`.\n",
    "\n",
    "9. **Memory Cleanup:**\n",
    "   - Unused variables (`grid_df`, `calendar_df`) are deleted to free up memory.\n",
    "   - Another call to `gc.collect()` performs garbage collection once again.\n",
    "\n",
    "##### Note:\n",
    "- The `reduce_mem_usage` function is likely a custom function used to reduce memory usage in the DataFrame by downcasting numerical data types.\n",
    "\n",
    "- The `get_moon_phase` function is not defined in this code snippet, so it must be defined elsewhere to calculate the moon phase based on the date.\n",
    "\n",
    "- The function assumes that the `calendar_df` provided has the necessary columns and data for merging and processing. Ensure that the input DataFrame is structured correctly and contains the required information before using this function.\n",
    "\n",
    "- The resulting `grid_df` will have the additional calendar and date-related features, making it suitable for further analysis, modeling, or time-series forecasting tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e936f901",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_grid_calendar(calendar_df, end_train_day_x, predict_horizon):\n",
    "    \n",
    "    grid_df = pd.read_feather(f\"grid_df_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather\")\n",
    "    grid_df = grid_df[['id', 'd']]\n",
    "    gc.collect()\n",
    "\n",
    "    calendar_df['moon'] = calendar_df.date.apply(get_moon_phase)\n",
    "\n",
    "    # Merge calendar partly\n",
    "    icols = ['date',\n",
    "             'd',\n",
    "             'event_name_1',\n",
    "             'event_type_1',\n",
    "             'event_name_2',\n",
    "             'event_type_2',\n",
    "             'snap_CA',\n",
    "             'snap_TX',\n",
    "             'snap_WI',\n",
    "             'moon',\n",
    "             ]\n",
    "\n",
    "    grid_df = grid_df.merge(calendar_df[icols], on=['d'], how='left')\n",
    "\n",
    "    icols = ['event_name_1',\n",
    "             'event_type_1',\n",
    "             'event_name_2',\n",
    "             'event_type_2',\n",
    "             'snap_CA',\n",
    "             'snap_TX',\n",
    "             'snap_WI']\n",
    "    \n",
    "    for col in icols:\n",
    "        grid_df[col] = grid_df[col].astype('category')\n",
    "\n",
    "    grid_df['date'] = pd.to_datetime(grid_df['date'])\n",
    "\n",
    "    grid_df['tm_d'] = grid_df['date'].dt.day.astype(np.int8)\n",
    "    grid_df['tm_w'] = grid_df['date'].dt.isocalendar().week.astype(np.int8)\n",
    "    grid_df['tm_m'] = grid_df['date'].dt.month.astype(np.int8)\n",
    "    grid_df['tm_y'] = grid_df['date'].dt.year\n",
    "    grid_df['tm_y'] = (grid_df['tm_y'] - grid_df['tm_y'].min()).astype(np.int8)\n",
    "    grid_df['tm_wm'] = grid_df['tm_d'].apply(lambda x: math.ceil(x / 7)).astype(np.int8)\n",
    "\n",
    "    grid_df['tm_dw'] = grid_df['date'].dt.dayofweek.astype(np.int8)\n",
    "    grid_df['tm_w_end'] = (grid_df['tm_dw'] >= 5).astype(np.int8)\n",
    "                                                         \n",
    "    del(grid_df['date'])\n",
    "    grid_df = reduce_mem_usage(grid_df, verbose=False)\n",
    "    grid_df.to_feather(f\"grid_calendar_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather\")\n",
    "                                                         \n",
    "    del(grid_df)\n",
    "    del(calendar_df)\n",
    "    gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "35770f82",
   "metadata": {},
   "source": [
    "### Function Explanation: modify_grid_base\n",
    "\n",
    "This function takes two parameters, `end_train_day_x` and `predict_horizon`, to modify and optimize a DataFrame named `grid_df`. The goal is to reduce memory usage while preserving relevant data for the specified time range.\n",
    "\n",
    "#### Parameters:\n",
    "- `end_train_day_x`: The last day of the training data range (an integer).\n",
    "- `predict_horizon`: The number of days to predict into the future (an integer).\n",
    "\n",
    "#### Steps:\n",
    "\n",
    "1. **Reading Data:** The function starts by reading the DataFrame `grid_df` from a Feather file that contains data for the time range from `end_train_day_x` to `end_train_day_x + predict_horizon`. The file is assumed to be in the format \"grid_df_end_train_day_x_to_end_train_day_x+predict_horizon.feather\".\n",
    "\n",
    "2. **Day Column Modification:** The 'd' column in `grid_df` contains day identifiers in the format \"d_x\", where 'x' represents the day number. The function applies a lambda function to each element of the 'd' column to remove the leading \"d_\" part and converts the remaining day number to a 16-bit integer. This step simplifies the day representation and reduces memory usage.\n",
    "\n",
    "3. **Removing Unnecessary Column:** The 'wm_yr_wk' column, which likely represents the week number, is not required for the analysis and is deleted from `grid_df` to further save memory.\n",
    "\n",
    "4. **Memory Optimization:** The function calls the `reduce_mem_usage` function (not shown) to optimize the memory usage of the DataFrame `grid_df`. This step aims to reduce the memory footprint while maintaining data integrity.\n",
    "\n",
    "5. **Saving Modified Data:** The function saves the modified DataFrame `grid_df` back to a new Feather file with the same naming convention as the input file. This ensures that the optimized data is preserved for future use.\n",
    "\n",
    "6. **Memory Cleanup:** The function then deletes the DataFrame `grid_df` and performs a garbage collection (`gc.collect()`) to release any unreferenced memory and free up system resources.\n",
    "\n",
    "#### Return:\n",
    "The function doesn't have a return value, as it performs in-place modifications on the DataFrame `grid_df`.\n",
    "\n",
    "Note: The `reduce_mem_usage` function called within this function is not shown in the provided code snippet, but it is presumably defined elsewhere to handle memory optimization.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea33d826",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-10T08:01:59.656788Z",
     "iopub.status.busy": "2022-08-10T08:01:59.656262Z",
     "iopub.status.idle": "2022-08-10T08:01:59.663597Z",
     "shell.execute_reply": "2022-08-10T08:01:59.662254Z"
    },
    "papermill": {
     "duration": 0.016255,
     "end_time": "2022-08-10T08:01:59.666143",
     "exception": false,
     "start_time": "2022-08-10T08:01:59.649888",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def modify_grid_base(end_train_day_x, predict_horizon):\n",
    "    grid_df = pd.read_feather(f\"grid_df_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather\")\n",
    "    grid_df['d'] = grid_df['d'].apply(lambda x: x[2:]).astype(np.int16)\n",
    "    del grid_df['wm_yr_wk']\n",
    "    \n",
    "    grid_df = reduce_mem_usage(grid_df, verbose=False)\n",
    "    grid_df.to_feather(f\"grid_df_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather\")\n",
    "    \n",
    "    del(grid_df)\n",
    "    gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d01f7aa2",
   "metadata": {},
   "source": [
    "### Function: `generate_lag_feature(end_train_day_x, predict_horizon)`\n",
    "\n",
    "This function generates lag features and rolling statistics for the sales data in a DataFrame. The generated features are used for time series forecasting and prediction tasks.\n",
    "\n",
    "**Arguments:**\n",
    "- `end_train_day_x`: An integer representing the last day of the training data. The function will generate lag features and rolling statistics up to this day.\n",
    "- `predict_horizon`: An integer representing the number of days into the future for which predictions are to be made.\n",
    "\n",
    "**Function Steps:**\n",
    "\n",
    "1. Read Data:\n",
    "   - The function reads a Feather file named `grid_df_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather`, containing the raw sales data for the specified time range. The data is stored in a DataFrame named `grid_df`.\n",
    "   - The DataFrame `grid_df` is then filtered to retain only the columns \"id\", \"d\", and \"sales\", discarding other irrelevant columns.\n",
    "\n",
    "2. Generate Lag Features:\n",
    "   - The function specifies a list of `num_lag_day_list` representing the number of lag days to be used for generating lag features. By default, it takes the 15 days before the `predict_horizon`.\n",
    "   - For each lag day value in `num_lag_day_list`, it computes the lag feature for the 'sales' column using the `shift()` function on grouped data, and assigns the result to a new column in `grid_df`.\n",
    "\n",
    "3. Data Type Optimization:\n",
    "   - The function iterates through the columns of `grid_df`, and if a column name contains 'lag', it converts the data type of that column to `np.float16`. This optimization helps reduce memory usage while preserving data integrity.\n",
    "\n",
    "4. Generate Rolling Statistics:\n",
    "   - The function specifies a list of `num_rolling_day_list` representing the window sizes for calculating rolling statistics. It computes the rolling mean and standard deviation for each window size using the `rolling()` function on grouped data, and assigns the results to new columns in `grid_df`.\n",
    "\n",
    "5. Memory Optimization:\n",
    "   - The function calls the `reduce_mem_usage()` function to optimize the memory usage of `grid_df`, reducing the memory footprint of the DataFrame.\n",
    "\n",
    "6. Save Results:\n",
    "   - The resulting DataFrame `grid_df` with the lag features and rolling statistics is saved to a new Feather file named `lag_feature_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather`.\n",
    "\n",
    "7. Clean Up:\n",
    "   - The function releases the memory occupied by `grid_df` using `del` and `gc.collect()` to free up resources.\n",
    "\n",
    "**Note:**\n",
    "- The function relies on the existence of a `reduce_mem_usage()` function to optimize DataFrame memory usage, which is not provided in the code snippet. Ensure you have this function defined or replace it with an appropriate memory optimization method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45695ff6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-10T08:01:59.677555Z",
     "iopub.status.busy": "2022-08-10T08:01:59.677168Z",
     "iopub.status.idle": "2022-08-10T08:01:59.690871Z",
     "shell.execute_reply": "2022-08-10T08:01:59.689775Z"
    },
    "papermill": {
     "duration": 0.022763,
     "end_time": "2022-08-10T08:01:59.693725",
     "exception": false,
     "start_time": "2022-08-10T08:01:59.670962",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_lag_feature(end_train_day_x, predict_horizon):\n",
    "    grid_df = pd.read_feather(f\"grid_df_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather\")\n",
    "    grid_df = grid_df[['id', 'd', 'sales']]\n",
    "    \n",
    "    num_lag_day_list = []\n",
    "    num_lag_day = 15\n",
    "    for col in range(predict_horizon, predict_horizon + num_lag_day):\n",
    "        num_lag_day_list.append(col)\n",
    "    \n",
    "    num_rolling_day_list = [7, 14, 30, 60, 180]\n",
    "    num_shift_rolling_day_list = []\n",
    "    for num_shift_day in [1, 7, 14]:\n",
    "        for num_rolling_day in [7, 14, 30, 60]:\n",
    "            num_shift_rolling_day_list.append([num_shift_day, num_rolling_day])\n",
    "   \n",
    "    grid_df = grid_df.assign(**{\n",
    "        '{}_lag_{}'.format(col, l): grid_df.groupby(['id'])['sales'].transform(lambda x: x.shift(l))\n",
    "        for l in num_lag_day_list\n",
    "    })\n",
    "\n",
    "    for col in list(grid_df):\n",
    "        if 'lag' in col:\n",
    "            grid_df[col] = grid_df[col].astype(np.float16)\n",
    "\n",
    "    for num_rolling_day in num_rolling_day_list:\n",
    "        grid_df['rolling_mean_' + str(num_rolling_day)] = grid_df.groupby(['id'])['sales'].transform(\n",
    "            lambda x: x.shift(predict_horizon).rolling(num_rolling_day).mean()).astype(np.float16)\n",
    "        grid_df['rolling_std_' + str(num_rolling_day)] = grid_df.groupby(['id'])['sales'].transform(\n",
    "            lambda x: x.shift(predict_horizon).rolling(num_rolling_day).std()).astype(np.float16)\n",
    "\n",
    "    grid_df = reduce_mem_usage(grid_df, verbose=False)\n",
    "    grid_df.to_feather(f\"lag_feature_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather\")\n",
    "    \n",
    "    del(grid_df)\n",
    "    gc.collect()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0b72069",
   "metadata": {},
   "source": [
    "### Function: generate_target_encoding_feature(end_train_day_x, predict_horizon)\n",
    "\n",
    "#### Purpose:\n",
    "This function generates target encoding features based on historical sales data for a given time period, defined by `end_train_day_x` and `predict_horizon`.\n",
    "\n",
    "#### Input Parameters:\n",
    "- `end_train_day_x`: The end day of the training data, represented as an integer (e.g., 100, 200).\n",
    "- `predict_horizon`: The number of days into the future to predict, represented as an integer (e.g., 7, 14).\n",
    "\n",
    "#### Function Flow:\n",
    "1. The function first reads a DataFrame `grid_df` from a Feather file named `grid_df_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather`. This file is expected to contain historical sales data for the specified time period.\n",
    "\n",
    "2. It sets the 'sales' values to NaN for the future days in `grid_df`, i.e., days after `end_train_day_x`, to prepare the target encoding features for future predictions.\n",
    "\n",
    "3. A list `icols` is defined, containing different combinations of columns to group by for calculating target encoding. These combinations represent different levels of granularity for the target encoding features.\n",
    "\n",
    "4. For each column combination `col` in `icols`, the function calculates the mean and standard deviation of 'sales' for each group and creates new columns 'enc_{col}_mean' and 'enc_{col}_std' to store the target encoding features.\n",
    "\n",
    "5. The function reduces the memory usage of the DataFrame `grid_df` to optimize memory consumption.\n",
    "\n",
    "6. The target encoding features along with the relevant 'id' and 'd' columns are stored in a new Feather file named `target_encoding_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather`.\n",
    "\n",
    "7. The function performs cleanup by deleting the DataFrame `grid_df` and calling the garbage collector to release unused memory.\n",
    "\n",
    "#### Notes:\n",
    "- Target encoding is a technique used in machine learning to transform categorical variables into numerical features based on the target variable (in this case, 'sales'). It helps capture relationships between categorical features and the target variable.\n",
    "- The function assumes the existence of a custom function `reduce_mem_usage()` that reduces the memory usage of the DataFrame by converting numerical columns to appropriate data types.\n",
    "\n",
    "#### Example Usage:\n",
    "```python\n",
    "# Generate target encoding features for the time period from day 100 to day 107\n",
    "generate_target_encoding_feature(end_train_day_x=100, predict_horizon=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4286fd26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-10T08:01:59.706081Z",
     "iopub.status.busy": "2022-08-10T08:01:59.705690Z",
     "iopub.status.idle": "2022-08-10T08:01:59.715759Z",
     "shell.execute_reply": "2022-08-10T08:01:59.714627Z"
    },
    "papermill": {
     "duration": 0.019216,
     "end_time": "2022-08-10T08:01:59.718182",
     "exception": false,
     "start_time": "2022-08-10T08:01:59.698966",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_target_encoding_feature(end_train_day_x, predict_horizon):\n",
    "\n",
    "    grid_df = pd.read_feather(f\"grid_df_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather\")\n",
    "    \n",
    "    grid_df.loc[grid_df['d'] > (end_train_day_x - predict_horizon), 'sales'] = np.nan\n",
    "    base_cols = list(grid_df)\n",
    "\n",
    "    icols = [\n",
    "        ['state_id'],\n",
    "        ['store_id'],\n",
    "        ['cat_id'],\n",
    "        ['dept_id'],\n",
    "        ['state_id', 'cat_id'],\n",
    "        ['state_id', 'dept_id'],\n",
    "        ['store_id', 'cat_id'],\n",
    "        ['store_id', 'dept_id'],\n",
    "        ['item_id'],\n",
    "        ['item_id', 'state_id'],\n",
    "        ['item_id', 'store_id']\n",
    "    ]\n",
    "\n",
    "    for col in icols:\n",
    "        col_name = '_' + '_'.join(col) + '_'\n",
    "        grid_df['enc' + col_name + 'mean'] = grid_df.groupby(col)['sales'].transform('mean').astype(\n",
    "            np.float16)\n",
    "        grid_df['enc' + col_name + 'std'] = grid_df.groupby(col)['sales'].transform('std').astype(\n",
    "            np.float16)\n",
    "\n",
    "    keep_cols = [col for col in list(grid_df) if col not in base_cols]\n",
    "    grid_df = grid_df[['id', 'd'] + keep_cols]\n",
    "\n",
    "    grid_df = reduce_mem_usage(grid_df, verbose=False)\n",
    "    grid_df.to_feather(f\"target_encoding_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather\")\n",
    "    \n",
    "    del(grid_df)\n",
    "    gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8513953",
   "metadata": {},
   "source": [
    "# Assemble Grid by Store Function\n",
    "\n",
    "The `assemble_grid_by_store` function is designed to assemble a new grid data for each store based on the input data from the `train_df` DataFrame. It combines multiple feather files containing different components of the grid data and performs some data manipulation and merging operations.\n",
    "\n",
    "### Parameters:\n",
    "\n",
    "- `train_df`: The input DataFrame containing the training data for different stores.\n",
    "- `end_train_day_x`: An integer representing the end day of the training data.\n",
    "- `predict_horizon`: An integer indicating the number of days into the future to predict.\n",
    "\n",
    "### Function Overview:\n",
    "\n",
    "1. **Concatenating DataFrames:**\n",
    "   The function starts by concatenating three feather files using `pd.concat`. These files contain different components of the grid data:\n",
    "   - A feather file containing the main grid data (`grid_df`).\n",
    "   - A feather file containing price data for products (`grid_price`).\n",
    "   - A feather file containing calendar data (`grid_calendar`).\n",
    "   The resulting DataFrame is stored in `grid_df`.\n",
    "\n",
    "2. **Creating a Store ID List:**\n",
    "   The function creates a list of unique store IDs from the 'store_id' column in `train_df`. This list will be used to process data for each store separately.\n",
    "\n",
    "3. **Extracting Data for Each Store:**\n",
    "   For each store ID in the list, the function extracts the corresponding data from `grid_df` and stores it in a dictionary called `index_store`. This dictionary stores the extracted data along with their original indices, which will be used later for merging data.\n",
    "\n",
    "4. **Saving Grid Data for Each Store:**\n",
    "   The function then iterates through the store ID list and processes data for each store individually. It loads the extracted grid data for the specific store from the dictionary, resets the index, and saves the data to a new feather file with a store-specific filename.\n",
    "\n",
    "5. **Target Encoding Features:**\n",
    "   The function loads the target encoding features (mean and standard deviation) from a separate feather file (`target_encoding`) and stores them in a DataFrame `df2`.\n",
    "\n",
    "6. **Merging Target Encoding Features:**\n",
    "   For each store ID, the function loads the corresponding grid data, merges it with the target encoding features for that store (using the pre-saved indices from `index_store`), and saves the updated data to a new feather file with a store-specific filename.\n",
    "\n",
    "7. **Lag Features:**\n",
    "   The function loads lag features data from a separate feather file (`lag_feature`) and stores them in a DataFrame `df3`.\n",
    "\n",
    "8. **Merging Lag Features:**\n",
    "   Similar to the previous step, the function loads the grid data for each store, merges it with the lag features for that store (using the pre-saved indices from `index_store`), and saves the updated data to a new feather file with a store-specific filename.\n",
    "\n",
    "### Memory Management:\n",
    "\n",
    "Throughout the function, the script uses `gc.collect()` to release memory used by intermediate data and DataFrames after they are no longer needed. This helps manage memory usage efficiently when dealing with large datasets.\n",
    "\n",
    "Overall, the function helps create separate grid data for each store by combining various components, merging target encoding and lag features, and saves the final grid data for each store into separate feather files for later use in modeling or analysis.\n",
    "\n",
    "Please note that the function assumes the existence of specific feather files with the appropriate filenames, which were likely created in earlier steps of the data preparation process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc4e3d4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-10T08:01:59.730751Z",
     "iopub.status.busy": "2022-08-10T08:01:59.730346Z",
     "iopub.status.idle": "2022-08-10T08:01:59.742370Z",
     "shell.execute_reply": "2022-08-10T08:01:59.741620Z"
    },
    "papermill": {
     "duration": 0.020692,
     "end_time": "2022-08-10T08:01:59.744432",
     "exception": false,
     "start_time": "2022-08-10T08:01:59.723740",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def assemble_grid_by_store(train_df, end_train_day_x, predict_horizon):\n",
    "    grid_df = pd.concat([pd.read_feather(f\"grid_df_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather\"),\n",
    "                     pd.read_feather(f\"grid_price_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather\").iloc[:, 2:],\n",
    "                     pd.read_feather(f\"grid_calendar_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather\").iloc[:, 2:]],\n",
    "                     axis=1)\n",
    "    gc.collect()\n",
    "    store_id_set_list = list(train_df['store_id'].unique())\n",
    "\n",
    "    index_store = dict()\n",
    "    for store_id in store_id_set_list:\n",
    "        extract = grid_df[grid_df['store_id'] == store_id]\n",
    "        index_store[store_id] = extract.index.to_numpy()\n",
    "        extract = extract.reset_index(drop=True)\n",
    "        extract.to_feather(f\"grid_full_store_{store_id}_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather\")\n",
    "\n",
    "    del(grid_df)\n",
    "    gc.collect()\n",
    "    \n",
    "    mean_features = [\n",
    "        'enc_cat_id_mean', 'enc_cat_id_std',\n",
    "        'enc_dept_id_mean', 'enc_dept_id_std',\n",
    "        'enc_item_id_mean', 'enc_item_id_std'\n",
    "        ]\n",
    "    df2 = pd.read_feather(f\"target_encoding_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather\")[mean_features]\n",
    "\n",
    "    for store_id in store_id_set_list:\n",
    "        df = pd.read_feather(f\"grid_full_store_{store_id}_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather\")\n",
    "        df = pd.concat([df, df2[df2.index.isin(index_store[store_id])].reset_index(drop=True)], axis=1)\n",
    "        df.to_feather(f\"grid_full_store_{store_id}_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather\")\n",
    "\n",
    "    del(df2)\n",
    "    gc.collect()\n",
    "    \n",
    "    df3 = pd.read_feather(f\"lag_feature_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather\").iloc[:, 3:]\n",
    "\n",
    "    for store_id in store_id_set_list:\n",
    "        df = pd.read_feather(f\"grid_full_store_{store_id}_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather\")\n",
    "        df = pd.concat([df, df3[df3.index.isin(index_store[store_id])].reset_index(drop=True)], axis=1)\n",
    "        df.to_feather(f\"grid_full_store_{store_id}_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather\")\n",
    "\n",
    "    del(df3)\n",
    "    del(store_id_set_list)\n",
    "    gc.collect()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb3317fd",
   "metadata": {},
   "source": [
    "### Function Explanation: load_grid_by_store\n",
    "\n",
    "This function loads data from a Feather file representing sales data for a specific store within a given time range. It performs some preprocessing and memory optimization before returning the final DataFrame and a list of enabled features.\n",
    "\n",
    "#### Parameters:\n",
    "- `end_train_day_x`: The last day of the training data for the store. (int)\n",
    "- `predict_horizon`: The number of days to predict into the future. (int)\n",
    "- `store_id`: The unique identifier of the store for which data is loaded. (str)\n",
    "\n",
    "#### Function Steps:\n",
    "1. **Read Data from Feather File:**\n",
    "   - The function reads the sales data for the specified store and time range from a Feather file named `grid_full_store_{store_id}_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather`. This file should contain columns like 'id', 'd', 'sales', and additional features.\n",
    "\n",
    "2. **Remove Unnecessary Features:**\n",
    "   - Some features like 'id', 'state_id', 'store_id', 'date', 'wm_yr_wk', 'd', and 'sales' are not needed for further processing or modeling. These features are stored in the `remove_features` list.\n",
    "   - The function creates a list of `enable_features` containing all the features from the DataFrame except the ones in the `remove_features` list.\n",
    "\n",
    "3. **Rearrange Columns and Reduce Memory Usage:**\n",
    "   - The DataFrame `df` is rearranged to have columns in the following order: 'id', 'd', 'sales', and then the remaining `enable_features`. This ensures that the target variable 'sales' and essential information about each row are placed at the beginning of the DataFrame.\n",
    "   - The function applies the `reduce_mem_usage` function to optimize memory usage in the DataFrame. This function reduces the memory footprint of numerical columns by using appropriate data types without compromising data integrity. It is likely a custom memory optimization function implemented elsewhere in the code.\n",
    "\n",
    "4. **Garbage Collection and Return:**\n",
    "   - The function performs garbage collection (`gc.collect()`) to free up any memory that is no longer in use after the DataFrame has been processed.\n",
    "   - Finally, the function returns the processed DataFrame `df` and the list of `enable_features`, which will be used in later stages of the data analysis or modeling process.\n",
    "\n",
    "#### Example Usage:\n",
    "```python\n",
    "store_data, features = load_grid_by_store(1913, 28, \"CA_1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39117ce7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-10T08:01:59.756411Z",
     "iopub.status.busy": "2022-08-10T08:01:59.756021Z",
     "iopub.status.idle": "2022-08-10T08:01:59.763197Z",
     "shell.execute_reply": "2022-08-10T08:01:59.761932Z"
    },
    "papermill": {
     "duration": 0.015746,
     "end_time": "2022-08-10T08:01:59.765475",
     "exception": false,
     "start_time": "2022-08-10T08:01:59.749729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_grid_by_store(end_train_day_x, predict_horizon, store_id):\n",
    "    df = pd.read_feather(f\"grid_full_store_{store_id}_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather\")\n",
    "                          \n",
    "    remove_features = ['id', 'state_id', 'store_id', 'date', 'wm_yr_wk', 'd', 'sales']\n",
    "    enable_features = [col for col in list(df) if col not in remove_features]\n",
    "    df = df[['id', 'd', 'sales'] + enable_features]\n",
    "    df = reduce_mem_usage(df, verbose=False)\n",
    "    gc.collect()\n",
    "                          \n",
    "    return df, enable_features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7cda9c2c",
   "metadata": {},
   "source": [
    "### Function: train(train_df, seed, end_train_day_x, predict_horizon)\n",
    "\n",
    "This function is used for training LightGBM models to predict sales for different store locations. It takes the following parameters:\n",
    "- `train_df`: The input DataFrame containing the training data.\n",
    "- `seed`: The random seed used for reproducibility in model training.\n",
    "- `end_train_day_x`: The last day of the training data, denoted as \"d_x\", where \"x\" is a day identifier.\n",
    "- `predict_horizon`: The prediction horizon, representing the number of days into the future to make predictions for.\n",
    "\n",
    "#### LightGBM Parameters:\n",
    "\n",
    "The function defines a dictionary `lgb_params` containing various hyperparameters for the LightGBM model. These parameters include the boosting type, objective function, learning rate, number of leaves, feature fraction, and more. The model is configured to use the \"tweedie\" objective with a variance power of 1.1 and \"goss\" boosting type for gradient-based one-side sampling.\n",
    "\n",
    "#### Random Seed and Environment Settings:\n",
    "\n",
    "The random seed and environment settings are initialized to ensure reproducibility in the training process across runs. The random seed is set for Python, NumPy, and LightGBM.\n",
    "\n",
    "#### Training Process:\n",
    "\n",
    "The function loops over each unique store ID in the input DataFrame `train_df`. For each store, it proceeds as follows:\n",
    "\n",
    "1. **Data Loading and Preparation:**\n",
    "   - The function calls `load_grid_by_store()` to load the data for the specific store based on `end_train_day_x` and `predict_horizon`. It also retrieves a list of enable features (columns) for the training.\n",
    "\n",
    "2. **Data Splitting:**\n",
    "   - The data is split into three parts: training, validation, and prediction. Training data includes rows where the day identifier is less than or equal to `end_train_day_x`. Validation data includes rows where the day identifier is within the last `predict_horizon` days of the training data. Prediction data is used for later predictions and includes rows with day identifiers beyond the last 100 days of the training data.\n",
    "\n",
    "3. **Model Training:**\n",
    "   - A LightGBM model is trained using the training data and validated using the validation data. The trained model is saved to a binary file named `lgb_model_{store_id}_{predict_horizon}.bin`.\n",
    "\n",
    "4. **Feature Importance:**\n",
    "   - The function calculates feature importance scores based on the trained model and stores them in a CSV file named `feature_importance_{store_id}_{predict_horizon}.csv`. These scores show the relative importance of each feature in predicting sales for the specific store.\n",
    "\n",
    "5. **Aggregating Feature Importance:**\n",
    "   - The feature importance scores for all stores are aggregated and saved in a CSV file named `feature_importance_all_{predict_horizon}.csv`. Additionally, a summary of mean and standard deviation of feature importance scores across all stores is saved to `feature_importance_agg_{predict_horizon}.csv`.\n",
    "\n",
    "The training process is repeated for each unique store, and at the end of the function execution, the trained models and feature importance information are available for each store, as well as their aggregation.\n",
    "\n",
    "The function's output includes trained models, feature importance files, and aggregated feature importance files, which can be used for further analysis, model evaluation, or making predictions on the test dataset.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8d9eb54a",
   "metadata": {},
   "source": [
    "For training time reasons, we just modified the boosting type, choosing to use Gradient-Based One-Side Sampling (GOSS) instead of Gradient Boosting Decision Tree (GBDT) because that can really speed up training without much loss in terms of performance. A good speed-up to the\n",
    "model is also provided by the subsample parameter and the feature fraction: at each learning step of the gradient boosting, only half of the examples and half of the features will be considered. \n",
    "The Tweedie loss, with a power value of 1.1 (hence with an underlying distribution closer to Poisson) seems particularly effective in modeling intermittent series (where zero sales prevail). The used metric is just the root mean squared error (there is no necessity to use a custom metric for representing the competition metric). We also use the force_row_wise parameter to save memory in the Kaggle notebook. All the other parameters are exactly the ones presented by Monsaraida in his solution (apart from the subsampling parameter that has been disabled because of its\n",
    "incompatibility with the goss boosting type)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7f9fbe8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-10T08:01:59.778445Z",
     "iopub.status.busy": "2022-08-10T08:01:59.778046Z",
     "iopub.status.idle": "2022-08-10T08:01:59.796266Z",
     "shell.execute_reply": "2022-08-10T08:01:59.795091Z"
    },
    "papermill": {
     "duration": 0.02795,
     "end_time": "2022-08-10T08:01:59.799225",
     "exception": false,
     "start_time": "2022-08-10T08:01:59.771275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(train_df, seed, end_train_day_x, predict_horizon):\n",
    "    \n",
    "    lgb_params = {\n",
    "            'boosting_type': 'goss',\n",
    "            'objective': 'tweedie',\n",
    "            'tweedie_variance_power': 1.1,\n",
    "            'metric': 'rmse',\n",
    "            #'subsample': 0.5, # incompatibility with the goss boosting type\n",
    "            #'subsample_freq': 1, # incompatibility with the goss boosting type\n",
    "            'learning_rate': 0.03,\n",
    "            'num_leaves': 2 ** 11 - 1,\n",
    "            'min_data_in_leaf': 2 ** 12 - 1,\n",
    "            'feature_fraction': 0.5,\n",
    "            'max_bin': 100,\n",
    "            'boost_from_average': False,\n",
    "            'num_boost_round': 1400,\n",
    "            'verbose': -1,\n",
    "            'num_threads': os.cpu_count(),\n",
    "            'force_row_wise': True,\n",
    "        }\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "    lgb_params['seed'] = seed\n",
    "\n",
    "    store_id_set_list = list(train_df['store_id'].unique())\n",
    "    print(f\"training stores: {store_id_set_list}\")\n",
    "    \n",
    "    feature_importance_all_df = pd.DataFrame()\n",
    "    for store_index, store_id in enumerate(store_id_set_list):\n",
    "        print(f'now training {store_id} store')\n",
    "\n",
    "        grid_df, enable_features = load_grid_by_store(end_train_day_x, predict_horizon, store_id)\n",
    "\n",
    "        train_mask = grid_df['d'] <= end_train_day_x\n",
    "        valid_mask = train_mask & (grid_df['d'] > (end_train_day_x - predict_horizon))\n",
    "        preds_mask = grid_df['d'] > (end_train_day_x - 100)\n",
    "\n",
    "        train_data = lgb.Dataset(grid_df[train_mask][enable_features],\n",
    "                                 label=grid_df[train_mask]['sales'])\n",
    "\n",
    "        valid_data = lgb.Dataset(grid_df[valid_mask][enable_features],\n",
    "                                 label=grid_df[valid_mask]['sales'])\n",
    "\n",
    "\n",
    "        # Saving part of the dataset for later predictions\n",
    "        # Removing features that we need to calculate recursively\n",
    "        grid_df = grid_df[preds_mask].reset_index(drop=True)\n",
    "        grid_df.to_feather(f'test_{store_id}_{predict_horizon}.feather')\n",
    "        del(grid_df)\n",
    "        gc.collect()\n",
    "        \n",
    "        estimator = lgb.train(lgb_params,\n",
    "                              train_data,\n",
    "                              valid_sets=[valid_data],\n",
    "                              callbacks=[lgb.log_evaluation(period=100, show_stdv=False)],\n",
    "                              )\n",
    "\n",
    "        model_name = str(f'lgb_model_{store_id}_{predict_horizon}.bin')\n",
    "        feature_importance_store_df = pd.DataFrame(sorted(zip(enable_features, estimator.feature_importance())),\n",
    "                                                   columns=['feature_name', 'importance'])\n",
    "        feature_importance_store_df = feature_importance_store_df.sort_values('importance', ascending=False)\n",
    "        feature_importance_store_df['store_id'] = store_id\n",
    "        feature_importance_store_df.to_csv(f'feature_importance_{store_id}_{predict_horizon}.csv', index=False)\n",
    "        feature_importance_all_df = pd.concat([feature_importance_all_df, feature_importance_store_df])\n",
    "        pickle.dump(estimator, open(model_name, 'wb'))\n",
    "\n",
    "        del([train_data, valid_data, estimator])\n",
    "        gc.collect()\n",
    "\n",
    "    feature_importance_all_df.to_csv(f'feature_importance_all_{predict_horizon}.csv', index=False)\n",
    "    feature_importance_agg_df = feature_importance_all_df.groupby('feature_name')['importance'].agg(['mean', 'std']).reset_index()\n",
    "    feature_importance_agg_df.columns = ['feature_name', 'importance_mean', 'importance_std']\n",
    "    feature_importance_agg_df = feature_importance_agg_df.sort_values('importance_mean', ascending=False)\n",
    "    feature_importance_agg_df.to_csv(f'feature_importance_agg_{predict_horizon}.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "756a7569",
   "metadata": {},
   "source": [
    "### Function: train_pipeline(train_df, prices_df, calendar_df, end_train_day_x_list, prediction_horizon_list)\n",
    "\n",
    "This function is the main pipeline that executes a series of data preparation and modeling steps for training a forecasting model. It takes the following parameters:\n",
    "- `train_df`: The DataFrame containing historical sales data, which is used as the training data for the model.\n",
    "- `prices_df`: The DataFrame containing historical price information for the products.\n",
    "- `calendar_df`: The DataFrame containing calendar information, such as dates and events.\n",
    "- `end_train_day_x_list`: A list of end points for the training data, each representing a specific day.\n",
    "- `prediction_horizon_list`: A list of prediction horizons, each representing the number of days to predict into the future.\n",
    "\n",
    "The function performs the following steps for each combination of `end_train_day_x` and `predict_horizon`:\n",
    "\n",
    "1. **Data Preparation:**\n",
    "   - `generate_base_grid(train_df, end_train_day_x, predict_horizon)`: Generates a base grid containing all possible combinations of products, stores, and days up to `end_train_day_x` and extends it to `predict_horizon` days into the future.\n",
    "   - `calc_release_week(prices_df, end_train_day_x, predict_horizon)`: Calculates the release week for each product, which represents the week when the product was introduced to the market, based on historical price information.\n",
    "   - `generate_grid_price(prices_df, calendar_df, end_train_day_x, predict_horizon)`: Generates additional features related to prices by merging historical price data with calendar information for the given time range.\n",
    "   - `generate_grid_calendar(calendar_df, end_train_day_x, predict_horizon)`: Generates calendar-related features by merging calendar information with the grid data for the given time range.\n",
    "   - `modify_grid_base(end_train_day_x, predict_horizon)`: Modifies the base grid by adding lag features and other relevant data to enhance the feature set.\n",
    "   - `generate_lag_feature(end_train_day_x, predict_horizon)`: Generates lag features by incorporating historical sales data for a specified number of days before the target day.\n",
    "   - `generate_target_encoding_feature(end_train_day_x, predict_horizon)`: Generates target encoding features, which involve using aggregated sales information for different categorical variables.\n",
    "   - `assemble_grid_by_store(train_df, end_train_day_x, predict_horizon)`: Assembles the final grid by combining the modified base grid with the historical sales data up to `end_train_day_x`.\n",
    "\n",
    "2. **Modelling:**\n",
    "   - `train(train_df, seed, end_train_day_x, predict_horizon)`: Calls a function to train a forecasting model using the assembled grid data, a random seed value (`seed`), and the specified `end_train_day_x` and `predict_horizon`.\n",
    "\n",
    "Note: The output of each step may modify or extend the original `train_df` DataFrame, allowing for a comprehensive feature set to be created before training the model.\n",
    "\n",
    "The function provides a convenient way to iterate through different combinations of end points and prediction horizons and carry out a series of data preparation steps, followed by model training for each combination.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "758eef43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-10T08:01:59.810950Z",
     "iopub.status.busy": "2022-08-10T08:01:59.810524Z",
     "iopub.status.idle": "2022-08-10T08:01:59.818604Z",
     "shell.execute_reply": "2022-08-10T08:01:59.817235Z"
    },
    "papermill": {
     "duration": 0.016928,
     "end_time": "2022-08-10T08:01:59.821258",
     "exception": false,
     "start_time": "2022-08-10T08:01:59.804330",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_pipeline(train_df, prices_df, calendar_df, end_train_day_x_list, prediction_horizon_list, seed):\n",
    "    \n",
    "    for end_train_day_x in end_train_day_x_list:\n",
    "        \n",
    "        for predict_horizon in prediction_horizon_list:\n",
    "            \n",
    "            print(f\"end training point day: {end_train_day_x} - prediction horizon: {predict_horizon} days\")\n",
    "\n",
    "            # Data preparation\n",
    "            generate_base_grid(train_df, end_train_day_x, predict_horizon)\n",
    "            calc_release_week(prices_df, end_train_day_x, predict_horizon)\n",
    "            generate_grid_price(prices_df, calendar_df, end_train_day_x, predict_horizon)\n",
    "            generate_grid_calendar(calendar_df, end_train_day_x, predict_horizon)\n",
    "            modify_grid_base(end_train_day_x, predict_horizon)\n",
    "            generate_lag_feature(end_train_day_x, predict_horizon)\n",
    "            generate_target_encoding_feature(end_train_day_x, predict_horizon)\n",
    "            assemble_grid_by_store(train_df, end_train_day_x, predict_horizon)\n",
    "\n",
    "            # Modelling\n",
    "            train(train_df, seed, end_train_day_x, predict_horizon)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c07db130",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-10T08:01:59.833760Z",
     "iopub.status.busy": "2022-08-10T08:01:59.833317Z",
     "iopub.status.idle": "2022-08-10T14:27:37.234076Z",
     "shell.execute_reply": "2022-08-10T14:27:37.230773Z"
    },
    "papermill": {
     "duration": 23137.415673,
     "end_time": "2022-08-10T14:27:37.242339",
     "exception": false,
     "start_time": "2022-08-10T08:01:59.826666",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end training point day: 1913 - prediction horizon: 7 days\n",
      "                              id        item_id    dept_id   cat_id store_id  \\\n",
      "0  HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
      "1  HOBBIES_1_002_CA_1_evaluation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n",
      "2  HOBBIES_1_003_CA_1_evaluation  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1   \n",
      "3  HOBBIES_1_004_CA_1_evaluation  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1   \n",
      "4  HOBBIES_1_005_CA_1_evaluation  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1   \n",
      "\n",
      "  state_id    d  sales  \n",
      "0       CA  d_1    0.0  \n",
      "1       CA  d_1    0.0  \n",
      "2       CA  d_1    0.0  \n",
      "3       CA  d_1    0.0  \n",
      "4       CA  d_1    0.0  \n",
      "training stores: ['CA_1', 'CA_2', 'CA_3', 'CA_4', 'TX_1', 'TX_2', 'TX_3', 'WI_1', 'WI_2', 'WI_3']\n",
      "now training CA_1 store\n",
      "[100]\tvalid_0's rmse: 1.922\n",
      "[200]\tvalid_0's rmse: 1.91129\n",
      "[300]\tvalid_0's rmse: 1.90334\n",
      "[400]\tvalid_0's rmse: 1.89908\n",
      "[500]\tvalid_0's rmse: 1.89361\n",
      "[600]\tvalid_0's rmse: 1.88909\n",
      "[700]\tvalid_0's rmse: 1.8848\n",
      "[800]\tvalid_0's rmse: 1.87793\n",
      "[900]\tvalid_0's rmse: 1.87262\n",
      "[1000]\tvalid_0's rmse: 1.86755\n",
      "[1100]\tvalid_0's rmse: 1.86246\n",
      "[1200]\tvalid_0's rmse: 1.85816\n",
      "[1300]\tvalid_0's rmse: 1.85234\n",
      "[1400]\tvalid_0's rmse: 1.84739\n",
      "now training CA_2 store\n",
      "[100]\tvalid_0's rmse: 1.83686\n",
      "[200]\tvalid_0's rmse: 1.79603\n",
      "[300]\tvalid_0's rmse: 1.78553\n",
      "[400]\tvalid_0's rmse: 1.77585\n",
      "[500]\tvalid_0's rmse: 1.76911\n",
      "[600]\tvalid_0's rmse: 1.76131\n",
      "[700]\tvalid_0's rmse: 1.75492\n",
      "[800]\tvalid_0's rmse: 1.74894\n",
      "[900]\tvalid_0's rmse: 1.74324\n",
      "[1000]\tvalid_0's rmse: 1.73779\n",
      "[1100]\tvalid_0's rmse: 1.7331\n",
      "[1200]\tvalid_0's rmse: 1.72877\n",
      "[1300]\tvalid_0's rmse: 1.72469\n",
      "[1400]\tvalid_0's rmse: 1.72029\n",
      "now training CA_3 store\n",
      "[100]\tvalid_0's rmse: 2.34169\n",
      "[200]\tvalid_0's rmse: 2.30982\n",
      "[300]\tvalid_0's rmse: 2.29995\n",
      "[400]\tvalid_0's rmse: 2.28498\n",
      "[500]\tvalid_0's rmse: 2.27338\n",
      "[600]\tvalid_0's rmse: 2.26389\n",
      "[700]\tvalid_0's rmse: 2.25451\n",
      "[800]\tvalid_0's rmse: 2.24817\n",
      "[900]\tvalid_0's rmse: 2.23985\n",
      "[1000]\tvalid_0's rmse: 2.23172\n",
      "[1100]\tvalid_0's rmse: 2.22599\n",
      "[1200]\tvalid_0's rmse: 2.21926\n",
      "[1300]\tvalid_0's rmse: 2.21234\n",
      "[1400]\tvalid_0's rmse: 2.20596\n",
      "now training CA_4 store\n",
      "[100]\tvalid_0's rmse: 1.39305\n",
      "[200]\tvalid_0's rmse: 1.38578\n",
      "[300]\tvalid_0's rmse: 1.38153\n",
      "[400]\tvalid_0's rmse: 1.37704\n",
      "[500]\tvalid_0's rmse: 1.37308\n",
      "[600]\tvalid_0's rmse: 1.36943\n",
      "[700]\tvalid_0's rmse: 1.36545\n",
      "[800]\tvalid_0's rmse: 1.3623\n",
      "[900]\tvalid_0's rmse: 1.35882\n",
      "[1000]\tvalid_0's rmse: 1.35562\n",
      "[1100]\tvalid_0's rmse: 1.35219\n",
      "[1200]\tvalid_0's rmse: 1.34888\n",
      "[1300]\tvalid_0's rmse: 1.34581\n",
      "[1400]\tvalid_0's rmse: 1.34315\n",
      "now training TX_1 store\n",
      "[100]\tvalid_0's rmse: 1.70072\n",
      "[200]\tvalid_0's rmse: 1.67814\n",
      "[300]\tvalid_0's rmse: 1.66937\n",
      "[400]\tvalid_0's rmse: 1.66265\n",
      "[500]\tvalid_0's rmse: 1.6574\n",
      "[600]\tvalid_0's rmse: 1.65121\n",
      "[700]\tvalid_0's rmse: 1.64664\n",
      "[800]\tvalid_0's rmse: 1.64226\n",
      "[900]\tvalid_0's rmse: 1.63739\n",
      "[1000]\tvalid_0's rmse: 1.63177\n",
      "[1100]\tvalid_0's rmse: 1.62746\n",
      "[1200]\tvalid_0's rmse: 1.62413\n",
      "[1300]\tvalid_0's rmse: 1.61942\n",
      "[1400]\tvalid_0's rmse: 1.61525\n",
      "now training TX_2 store\n",
      "[100]\tvalid_0's rmse: 1.70713\n",
      "[200]\tvalid_0's rmse: 1.70093\n",
      "[300]\tvalid_0's rmse: 1.69095\n",
      "[400]\tvalid_0's rmse: 1.68192\n",
      "[500]\tvalid_0's rmse: 1.67489\n",
      "[600]\tvalid_0's rmse: 1.66921\n",
      "[700]\tvalid_0's rmse: 1.66347\n",
      "[800]\tvalid_0's rmse: 1.65857\n",
      "[900]\tvalid_0's rmse: 1.65309\n",
      "[1000]\tvalid_0's rmse: 1.64909\n",
      "[1100]\tvalid_0's rmse: 1.64389\n",
      "[1200]\tvalid_0's rmse: 1.64006\n",
      "[1300]\tvalid_0's rmse: 1.63512\n",
      "[1400]\tvalid_0's rmse: 1.63204\n",
      "now training TX_3 store\n",
      "[100]\tvalid_0's rmse: 1.78911\n",
      "[200]\tvalid_0's rmse: 1.78157\n",
      "[300]\tvalid_0's rmse: 1.77597\n",
      "[400]\tvalid_0's rmse: 1.76959\n",
      "[500]\tvalid_0's rmse: 1.76115\n",
      "[600]\tvalid_0's rmse: 1.75443\n",
      "[700]\tvalid_0's rmse: 1.748\n",
      "[800]\tvalid_0's rmse: 1.74221\n",
      "[900]\tvalid_0's rmse: 1.73663\n",
      "[1000]\tvalid_0's rmse: 1.73075\n",
      "[1100]\tvalid_0's rmse: 1.7255\n",
      "[1200]\tvalid_0's rmse: 1.71966\n",
      "[1300]\tvalid_0's rmse: 1.71509\n",
      "[1400]\tvalid_0's rmse: 1.71024\n",
      "now training WI_1 store\n",
      "[100]\tvalid_0's rmse: 1.57283\n",
      "[200]\tvalid_0's rmse: 1.56168\n",
      "[300]\tvalid_0's rmse: 1.55246\n",
      "[400]\tvalid_0's rmse: 1.54435\n",
      "[500]\tvalid_0's rmse: 1.53751\n",
      "[600]\tvalid_0's rmse: 1.53248\n",
      "[700]\tvalid_0's rmse: 1.52703\n",
      "[800]\tvalid_0's rmse: 1.52115\n",
      "[900]\tvalid_0's rmse: 1.51746\n",
      "[1000]\tvalid_0's rmse: 1.51241\n",
      "[1100]\tvalid_0's rmse: 1.5074\n",
      "[1200]\tvalid_0's rmse: 1.50265\n",
      "[1300]\tvalid_0's rmse: 1.49968\n",
      "[1400]\tvalid_0's rmse: 1.49494\n",
      "now training WI_2 store\n",
      "[100]\tvalid_0's rmse: 2.33396\n",
      "[200]\tvalid_0's rmse: 2.28773\n",
      "[300]\tvalid_0's rmse: 2.26648\n",
      "[400]\tvalid_0's rmse: 2.25313\n",
      "[500]\tvalid_0's rmse: 2.23966\n",
      "[600]\tvalid_0's rmse: 2.22832\n",
      "[700]\tvalid_0's rmse: 2.21672\n",
      "[800]\tvalid_0's rmse: 2.2087\n",
      "[900]\tvalid_0's rmse: 2.19942\n",
      "[1000]\tvalid_0's rmse: 2.1907\n",
      "[1100]\tvalid_0's rmse: 2.18304\n",
      "[1200]\tvalid_0's rmse: 2.17562\n",
      "[1300]\tvalid_0's rmse: 2.16776\n",
      "[1400]\tvalid_0's rmse: 2.15877\n",
      "now training WI_3 store\n",
      "[100]\tvalid_0's rmse: 1.83288\n",
      "[200]\tvalid_0's rmse: 1.76137\n",
      "[300]\tvalid_0's rmse: 1.74368\n",
      "[400]\tvalid_0's rmse: 1.73435\n",
      "[500]\tvalid_0's rmse: 1.72694\n",
      "[600]\tvalid_0's rmse: 1.7195\n",
      "[700]\tvalid_0's rmse: 1.71364\n",
      "[800]\tvalid_0's rmse: 1.71036\n",
      "[900]\tvalid_0's rmse: 1.70404\n",
      "[1000]\tvalid_0's rmse: 1.69732\n",
      "[1100]\tvalid_0's rmse: 1.69027\n",
      "[1200]\tvalid_0's rmse: 1.68611\n",
      "[1300]\tvalid_0's rmse: 1.68212\n",
      "[1400]\tvalid_0's rmse: 1.67701\n"
     ]
    }
   ],
   "source": [
    "end_train_day_x_list = [1913] # [1941, 1913, 1885, 1857, 1829, 1577]\n",
    "prediction_horizon_list = [7] # [7, 14, 21, 28]\n",
    "seed = 42\n",
    "\n",
    "train_pipeline(train_df, prices_df, calendar_df, end_train_day_x_list, prediction_horizon_list, seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "85d4f311",
   "metadata": {},
   "source": [
    "After defining the training parameters, we just iterate over the stores, each time uploading the\n",
    "training data of a single store and training the LightGBM model. Each model is then pickle dumped\n",
    "(saved). We also extract feature importance from each model in order to consolidate it into a\n",
    "file and then aggregate it, resulting in having the mean importance across all the stores for that\n",
    "prediction horizon for each feature."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07eeb5fb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m5-forecast",
   "language": "python",
   "name": "m5-forecast"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 23326.111899,
   "end_time": "2022-08-10T14:27:40.555555",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-08-10T07:58:54.443656",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
